{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac20953-34df-47fc-933b-fb532cbed39e",
   "metadata": {},
   "source": [
    "# Link prediction - (Neighborsampler. wrong one) \n",
    "- Based on this questions\n",
    "    - https://github.com/pyg-team/pytorch_geometric/discussions/6098\n",
    "- and this tutorial:\n",
    "    - https://colab.research.google.com/drive/1xpzn1Nvai1ygd_P5Yambc_oe4VBPK_ZT?usp=sharing\n",
    "- and this runthrough\n",
    "    - https://medium.com/@pytorch_geometric/link-prediction-on-heterogeneous-graphs-with-pyg-6d5c29677c70]\n",
    "----    \n",
    "-- Actually 2.2.0 tagged version https://github.com/pyg-team/pytorch_geometric/tree/2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3611a8-dd7a-4047-a18a-e7491ab2029b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "print(torch.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# note it uses the wrong linkNeighborLoader (there are two). one for sampler and one for loader.\n",
    "from torch_geometric.loader import LinkNeighborLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dae8d1-bb08-4901-8e6c-d8031abc3cb3",
   "metadata": {},
   "source": [
    "## Prepare Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f8c804-0378-4c13-a5f2-ad4e9725d49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "feature_store = HeteroData()\n",
    "graph_store = HeteroData()\n",
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64d6d3-76ec-4fdf-9527-903a005ba807",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6589cabc-69d9-479a-820d-99f7b8555de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting ./ml-latest-small.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "movies_path = './ml-latest-small/movies.csv'\n",
    "ratings_path = './ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bd9952-7273-4803-a18b-b36bcdc1ab2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies.csv:\n",
      "===========\n",
      "   movieId                                       genres\n",
      "0        1  Adventure|Animation|Children|Comedy|Fantasy\n",
      "1        2                   Adventure|Children|Fantasy\n",
      "2        3                               Comedy|Romance\n",
      "3        4                         Comedy|Drama|Romance\n",
      "4        5                                       Comedy\n",
      "\n",
      "ratings.csv:\n",
      "============\n",
      "   userId  movieId\n",
      "0       1        1\n",
      "1       1        3\n",
      "2       1        6\n",
      "3       1       47\n",
      "4       1       50\n"
     ]
    }
   ],
   "source": [
    "print('movies.csv:')\n",
    "print('===========')\n",
    "print(pd.read_csv(movies_path)[[\"movieId\", \"genres\"]].head())\n",
    "print()\n",
    "print('ratings.csv:')\n",
    "print('============')\n",
    "print(pd.read_csv(ratings_path)[[\"userId\", \"movieId\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4da9d3e-5183-42ee-ad90-23f883ea5299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Action  Adventure  Drama  Horror\n",
      "movieId                                  \n",
      "1             0          1      0       0\n",
      "2             0          1      0       0\n",
      "3             0          0      0       0\n",
      "4             0          0      1       0\n",
      "5             0          0      0       0\n"
     ]
    }
   ],
   "source": [
    "# Load the entire movie data frame into memory:\n",
    "movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
    "\n",
    "# Split genres and convert into indicator variables:\n",
    "genres = movies_df['genres'].str.get_dummies('|')\n",
    "print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "\n",
    "# Use genres as movie input features:\n",
    "movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "assert movie_feat.size() == (9742, 20)  # 20 genres in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c1dd78e-1d77-48d6-aca7-ed3a7dce41ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "   userId  mappedID\n",
      "0       1         0\n",
      "1       2         1\n",
      "2       3         2\n",
      "3       4         3\n",
      "4       5         4\n",
      "\n",
      "Mapping of movie IDs to consecutive values:\n",
      "===========================================\n",
      "   movieId  mappedID\n",
      "0        1         0\n",
      "1        3         1\n",
      "2        6         2\n",
      "3       47         3\n",
      "4       50         4\n",
      "\n",
      "Final edge indices pointing from users to movies:\n",
      "=================================================\n",
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    1,    2,  ..., 3121, 1392, 2873]])\n"
     ]
    }
   ],
   "source": [
    "# Load the entire ratings data frame into memory:\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "\n",
    "# Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "unique_user_id = ratings_df['userId'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'userId': unique_user_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "})\n",
    "print(\"Mapping of user IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_user_id.head())\n",
    "print()\n",
    "# Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "unique_movie_id = ratings_df['movieId'].unique()\n",
    "unique_movie_id = pd.DataFrame(data={\n",
    "    'movieId': unique_movie_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_movie_id)),\n",
    "})\n",
    "print(\"Mapping of movie IDs to consecutive values:\")\n",
    "print(\"===========================================\")\n",
    "print(unique_movie_id.head())\n",
    "\n",
    "# Perform merge to obtain the edges from users and movies:\n",
    "ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                            left_on='userId', right_on='userId', how='left')\n",
    "ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "ratings_movie_id = pd.merge(ratings_df['movieId'], unique_movie_id,\n",
    "                            left_on='movieId', right_on='movieId', how='left')\n",
    "ratings_movie_id = torch.from_numpy(ratings_movie_id['mappedID'].values)\n",
    "\n",
    "# With this, we are ready to construct our `edge_index` in COO format\n",
    "# following PyG semantics:\n",
    "edge_index_user_to_movie = torch.stack([ratings_user_id, ratings_movie_id], dim=0)\n",
    "assert edge_index_user_to_movie.size() == (2, 100836)\n",
    "\n",
    "print()\n",
    "print(\"Final edge indices pointing from users to movies:\")\n",
    "print(\"=================================================\")\n",
    "print(edge_index_user_to_movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e463d708-0f9c-4213-8604-91edfcffbc6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    1,    2,  ..., 3121, 1392, 2873])\n"
     ]
    }
   ],
   "source": [
    "print(ratings_movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151701b9-ab70-4ce9-a8cd-01424cc5d301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ node_id=[610] },\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    node_id=[9742],\n",
      "    x=[9742, 20]\n",
      "  },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={ edge_index=[2, 100836] },\n",
      "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 100836] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Save node indices:\n",
    "data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "data[\"movie\"].node_id = torch.arange(len(movies_df))\n",
    "\n",
    "# Add the node features and edge indices:\n",
    "data[\"movie\"].x = movie_feat # TODO\n",
    "data[\"user\", \"rates\", \"movie\"].edge_index = edge_index_user_to_movie  # TODO\n",
    "\n",
    "# We also need to make sure to add the reverse edges from movies to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "\n",
    "# TODO:\n",
    "# FROM HERE https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html\n",
    "transform = T.Compose([T.ToUndirected()])\n",
    "data = transform(data)          \n",
    "\n",
    "feature_store = HeteroData()\n",
    "graph_store = HeteroData()\n",
    "\n",
    "# Add the data to the feature store\n",
    "feature_store.put_tensor(movie_feat, group_name='movie', attr_name='x', index=torch.arange(len(movies_df)))\n",
    "\n",
    "# Add graph to graph store\n",
    "coo = (edge_index_user_to_movie[0], edge_index_user_to_movie[1])\n",
    "\n",
    "graph_store.put_edge_index(edge_index=coo,\n",
    "                           edge_type=('user', 'rates', 'movie'),\n",
    "                           layout='coo', size=(len(coo[0]),len(coo[1])) )\n",
    "\n",
    "# put reverse index\n",
    "graph_store.put_edge_index(edge_index=coo[::-1],\n",
    "                           edge_type=('movie', 'rev_rates', 'user'),\n",
    "                           layout='coo', size=(len(coo[1]),len(coo[0])) )\n",
    "\n",
    "\n",
    "#raise NotImplementedError\n",
    "\n",
    "print(data)\n",
    "\n",
    "assert data.node_types == [\"user\", \"movie\"]\n",
    "assert data.edge_types == [(\"user\", \"rates\", \"movie\"),\n",
    "                           (\"movie\", \"rev_rates\", \"user\")]\n",
    "assert data[\"user\"].num_nodes == 610\n",
    "assert data[\"user\"].num_features == 0\n",
    "assert data[\"movie\"].num_nodes == 9742\n",
    "assert data[\"movie\"].num_features == 20\n",
    "assert data[\"user\", \"rates\", \"movie\"].num_edges == 100836\n",
    "assert data[\"movie\", \"rev_rates\", \"user\"].num_edges == 100836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "663d6b37-9c3b-4bf0-970a-723e269c0345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ node_id=[610] },\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    node_id=[9742],\n",
      "    x=[9742, 20]\n",
      "  },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 56469],\n",
      "    edge_label=[24201],\n",
      "    edge_label_index=[2, 24201]\n",
      "  },\n",
      "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 56469] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ node_id=[610] },\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    node_id=[9742],\n",
      "    x=[9742, 20]\n",
      "  },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 80670],\n",
      "    edge_label=[30249],\n",
      "    edge_label_index=[2, 30249]\n",
      "  },\n",
      "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 80670] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# For this, we first split the set of edges into\n",
    "# training (80%), validation (10%), and testing edges (10%).\n",
    "# Across the training edges, we use 70% of edges for message passing,\n",
    "# and 30% of edges for supervision.\n",
    "# We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "# Negative edges during training will be generated on-the-fly.\n",
    "# We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, \n",
    "    num_test=0.1,  \n",
    "    disjoint_train_ratio=.3,  \n",
    "    neg_sampling_ratio=2,  \n",
    "    add_negative_train_samples=False,  \n",
    "    edge_types=(\"user\", \"rates\", \"movie\"),\n",
    "    rev_edge_types=(\"movie\", \"rev_rates\", \"user\"), \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)\n",
    "\n",
    "assert train_data[\"user\", \"rates\", \"movie\"].num_edges == 56469\n",
    "assert train_data[\"user\", \"rates\", \"movie\"].edge_label_index.size(1) == 24201\n",
    "assert train_data[\"movie\", \"rev_rates\", \"user\"].num_edges == 56469\n",
    "# No negative edges added:\n",
    "assert train_data[\"user\", \"rates\", \"movie\"].edge_label.min() == 1\n",
    "assert train_data[\"user\", \"rates\", \"movie\"].edge_label.max() == 1\n",
    "\n",
    "assert val_data[\"user\", \"rates\", \"movie\"].num_edges == 80670\n",
    "assert val_data[\"user\", \"rates\", \"movie\"].edge_label_index.size(1) == 30249\n",
    "assert val_data[\"movie\", \"rev_rates\", \"user\"].num_edges == 80670\n",
    "# Negative edges with ratio 2:1:\n",
    "assert val_data[\"user\", \"rates\", \"movie\"].edge_label.long().bincount().tolist() == [20166, 10083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969d3db-83e8-47e8-85a3-0b4e02f2d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we need to implement transform RandomLinkSplit\n",
    "# However it has not been thought through with feature store... \n",
    "# the problem is we now have 3 objects with similar names (our feature store does not understand dataset types). \n",
    "# So either we would need to Create 3 instances of features store (easiest, but wasteful)\n",
    "# or we need to add some sort of filter object or similar to the feature store / do a lot of re-engineering of all functions. \n",
    "# (right thing to do, but feels almost as a task / questions for PyG?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dacdd3d-e5f3-4a0b-ad3e-0cd26956501b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "phmap at(): lookup non-existent key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m LinkNeighborLoader(\n\u001b[1;32m      6\u001b[0m     data\u001b[38;5;241m=\u001b[39m(feature_store,graph_store),  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     num_neighbors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m],  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Inspect a sample:\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sampled_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampled mini-batch:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch_geometric/loader/base.py:36\u001b[0m, in \u001b[0;36mDataLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_fn(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch_geometric/loader/link_loader.py:171\u001b[0m, in \u001b[0;36mLinkLoader.collate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a subgraph from a batch of input nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m input_data: EdgeSamplerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[index]\n\u001b[0;32m--> 171\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlink_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_edges\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn(out)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py:345\u001b[0m, in \u001b[0;36mNeighborSampler.sample_from_edges\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_from_edges\u001b[39m(\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    342\u001b[0m     index: EdgeSamplerInput,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43medge_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_src_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dst_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisjoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnode_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py:494\u001b[0m, in \u001b[0;36medge_sample\u001b[0;34m(index, sample_fn, num_src_nodes, num_dst_nodes, disjoint, input_type, node_time, neg_sampling)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edge_label_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Always disjoint.\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         seed_time_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    491\u001b[0m             input_type[\u001b[38;5;241m0\u001b[39m]: torch\u001b[38;5;241m.\u001b[39mcat([src_time, dst_time], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    492\u001b[0m         }\n\u001b[0;32m--> 494\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_time_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Enhance `out` by label information ##################################\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disjoint:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py:258\u001b[0m, in \u001b[0;36mNeighborSampler._sample\u001b[0;34m(self, seed, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisjoint:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisjoint\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sampling not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneighbor sampling via \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyg-lib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for improved \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand optimized sampling routines.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhetero_neighbor_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolptr_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seed_dict\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     node, row, col, edge, batch \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HeteroSamplerOutput(\n\u001b[1;32m    272\u001b[0m     node\u001b[38;5;241m=\u001b[39mnode,\n\u001b[1;32m    273\u001b[0m     row\u001b[38;5;241m=\u001b[39mremap_keys(row, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_edge_type),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m    277\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/torch/lib/python3.8/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: phmap at(): lookup non-existent key"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define seed edges:\n",
    "edge_label_index = graph_store[\"user\", \"rates\", \"movie\"].edge_index\n",
    "edge_label = torch.ones( 1,graph_store[\"user\", \"rates\", \"movie\"].edge_index.size()[1]).flatten()\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=(feature_store,graph_store),  # TODO\n",
    "    num_neighbors=[20,10],  # TODO\n",
    "    neg_sampling_ratio=2,  # TODO\n",
    "    edge_label_index=((\"user\", \"rates\", \"movie\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Inspect a sample:\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)\n",
    "\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label_index.size(1) == 3 * 128\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label.min() == 0\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label.max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9267d6da-59f3-4dd5-bf55-39f6001bf397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (movie_lin): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (user_emb): Embedding(610, 64)\n",
      "  (movie_emb): Embedding(9742, 64)\n",
      "  (gnn): GraphModule(\n",
      "    (conv1): ModuleDict(\n",
      "      (user__rates__movie): SAGEConv(64, 64, aggr=mean)\n",
      "      (movie__rev_rates__user): SAGEConv(64, 64, aggr=mean)\n",
      "    )\n",
      "    (conv2): ModuleDict(\n",
      "      (user__rates__movie): SAGEConv(64, 64, aggr=mean)\n",
      "      (movie__rev_rates__user): SAGEConv(64, 64, aggr=mean)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "         # Define a 2-layer GNN computation graph.\n",
    "        # Use a *single* `ReLU` non-linearity in-between.\n",
    "        # TODO:\n",
    "        # https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "                   \n",
    "\n",
    "# Our final classifier applies the dot-product between source and destination\n",
    "# node embeddings to derive edge-level predictions:\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_user: Tensor, x_movie: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_user = x_user[edge_label_index[0]]\n",
    "        edge_feat_movie = x_movie[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_user * edge_feat_movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "        self.movie_lin = torch.nn.Linear(20, hidden_channels)\n",
    "        self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "        self.movie_emb = torch.nn.Embedding(data[\"movie\"].num_nodes, hidden_channels)\n",
    "\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "          \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb(data[\"movie\"].node_id),\n",
    "        } \n",
    "\n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"user\"],\n",
    "            x_dict[\"movie\"],\n",
    "            data[\"user\", \"rates\", \"movie\"].edge_label_index,\n",
    "        )\n",
    "\n",
    "        return pred\n",
    "\n",
    "        \n",
    "model = Model(hidden_channels=64)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0629f2ab-473b-4cfb-b6c3-c343da87a062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cpu'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 190/190 [00:10<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 190/190 [00:10<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.3502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 190/190 [00:10<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 190/190 [00:10<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 190/190 [00:10<00:00, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    total_loss = total_examples = 0\n",
    "    for sampled_data in tqdm.tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Move `sampled_data` to the respective `device`\n",
    "        # TODO: Run `forward` pass of the model\n",
    "        # TODO: Apply binary cross entropy via\n",
    "        # `F.binary_cross_entropy_with_logits(pred, ground_truth)`\n",
    "        sampled_data.to(device)\n",
    "        pred = model(sampled_data)\n",
    "        ground_truth = sampled_data[\"user\", \"rates\", \"movie\"].edge_label\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
    "                        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.numel()\n",
    "        total_examples += pred.numel()\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "614eeaaa-371c-4300-b7f2-964126076e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ node_id=[606] },\n",
      "  \u001b[1mmovie\u001b[0m={\n",
      "    node_id=[2664],\n",
      "    x=[2664, 20]\n",
      "  },\n",
      "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
      "    edge_index=[2, 19582],\n",
      "    edge_label=[384],\n",
      "    edge_label_index=[2, 384],\n",
      "    input_id=[384]\n",
      "  },\n",
      "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 7716] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the validation seed edges:\n",
    "edge_label_index = val_data[\"user\", \"rates\", \"movie\"].edge_label_index\n",
    "edge_label = val_data[\"user\", \"rates\", \"movie\"].edge_label\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data=val_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"rates\", \"movie\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=3 * 128,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(val_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)\n",
    "\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label_index.size(1) == 3 * 128\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label.min() >= 0\n",
    "assert sampled_data[\"user\", \"rates\", \"movie\"].edge_label.max() <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbc7ea95-e410-433a-a378-a4efd4f76228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation AUC: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "preds = []\n",
    "ground_truths = []\n",
    "for sampled_data in tqdm.tqdm(val_loader):\n",
    "    with torch.no_grad():\n",
    "        # TODO: Collect predictions and ground-truths and write them into\n",
    "        # `preds` and `ground_truths`.\n",
    "        sampled_data.to(device)\n",
    "        preds.append(model(sampled_data))\n",
    "        ground_truths.append(sampled_data[\"user\", \"rates\", \"movie\"].edge_label)\n",
    "\n",
    "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "auc = roc_auc_score(ground_truth, pred)\n",
    "print()\n",
    "print(f\"Validation AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39aa67-d23e-4406-ae81-0b602e80ad84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
